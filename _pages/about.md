---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Motto. Etc.

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Berkeley, CA, 94720</p>

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a first-year CS PhD student at UC Berkeley advised by Prof. [Bin Yu](https://binyu.stat.berkeley.edu/) and Prof. [Ion Stoica](https://people.eecs.berkeley.edu/~istoica/). I'm affiliated with [Sky Computing Lab](https://sky.cs.berkeley.edu/) and [BAIR](https://bair.berkeley.edu/). I completed my Master’s degree in Computational Science and Engineering at Harvard University, where I was fortunate to be advised by Prof. [Hima Lakkaraju](https://himalakkaraju.github.io). Prior to that, I earned my Bachelor’s degree from UC Berkeley, double majoring in Computer Science and Psychology.

My research centers on LLM evaluation, alignment, and safety, with an emphasis on direct practical impact and domain-specific applications. I’m also interested in foundational topics about how LLMs work, including mechanistic interpretability and reasoning.

Here are several overarching goals I hope my research projects aim to achieve:

(1) Propose novel frameworks for evaluating LLM capabilities that move beyond traditional benchmark-style tasks. These frameworks should better reflect real-world use cases, have tangible practical impact, and provide insights that inform future efforts in model development and alignment.

(2) Develop LLM-powered agents and systems that are both robust and adaptable, capable of fundamentally understanding and responding to domain shifts.

(3) Systematically characterize and unify diverse human-defined LLM behaviors across different levels of granularity. Many of these behaviors overlap or connect, suggesting the possibility of a tree-like hierarchy. Ideally, these behaviors should be studied through a hierarchical framework that enables unified observation, evaluation, and interpretation, instead of treating them as disconnected properties.

<!-- Here are a couple of miscellaneous thoughts currently in my mind that might lead to potentially interesting projects (if you are interested, please feel free to reach out):

(1) Meta-evaluation: 

(2) The asymmetry between an LLM's ability in providing valid examples and verification: When a model is presented with a set of conditions and asked to provide examples, what would its "thought-process" be like? Can we reduce the biases present in this process? -->

I'm always open to collaborations and happy to discuss all kinds of research ideas. The best way to contact me is through [email](mailto:aaronjli@berkeley.edu).